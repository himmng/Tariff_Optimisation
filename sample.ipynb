{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f82f9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88fb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: get or simulate some data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f95c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved billing_system_data.csv\n",
      "Saved advanced_interval_data.csv\n",
      "Saved regulatory_reporting.csv\n",
      "Saved technology_adoption.csv\n",
      "Saved network_load_profiles.csv\n",
      "All synthetic data generated in: synthetic_synergy_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUTPUT_DIR = \"synthetic_synergy_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PARAMETERS\n",
    "# ------------------------------------------------------------\n",
    "N_CUSTOMERS = 50_000      # change to 1_170_000 when needed\n",
    "N_SMART_METERS = 12_000   # subset for interval data\n",
    "N_YEARS = 3\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# A. BILLING SYSTEM DATA\n",
    "# ============================================================\n",
    "\n",
    "def simulate_billing_system_data():\n",
    "    tariffs = [\"A1\", \"L1\", \"L3\", \"R1\", \"R3\", \"M1\", \"K1\"]\n",
    "    customer_types = [\"residential\", \"small_business\", \"medium_business\", \"large_business\", \"home_business\"]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"customer_id\": np.arange(1, N_CUSTOMERS + 1),\n",
    "        \"postcode\": np.random.randint(6000, 6999, N_CUSTOMERS),\n",
    "        \"suburb_code\": np.random.randint(1, 500, N_CUSTOMERS),\n",
    "        \"tariff\": np.random.choice(tariffs, N_CUSTOMERS),\n",
    "        \"customer_type\": np.random.choice(customer_types, N_CUSTOMERS),\n",
    "        \"monthly_consumption_kwh\": np.random.gamma(shape=2.0, scale=200, size=N_CUSTOMERS),  # realistic heavy-tail\n",
    "        \"bill_amount\": np.random.uniform(80, 600, N_CUSTOMERS),\n",
    "        \"payment_history_score\": np.random.uniform(0, 1, N_CUSTOMERS),\n",
    "        \"concession_status\": np.random.choice([0, 1], N_CUSTOMERS, p=[0.85, 0.15]),\n",
    "        \"connection_date\": pd.date_range(\"2010-01-01\", periods=N_CUSTOMERS).to_series().sample(N_CUSTOMERS).values,\n",
    "    })\n",
    "\n",
    "    df[\"customer_tenure_years\"] = (pd.Timestamp(\"2025-01-01\") - pd.to_datetime(df[\"connection_date\"])).dt.days / 365\n",
    "\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/billing_system_data.csv\", index=False)\n",
    "    print(\"Saved billing_system_data.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# B. ADVANCED METER INTERVAL DATA (15/30 MIN)\n",
    "# ============================================================\n",
    "\n",
    "def simulate_interval_data(customer_ids):\n",
    "    interval_records = []\n",
    "\n",
    "    for cid in customer_ids:\n",
    "        for day in range(0, 7):  # one week simulation, scale if needed\n",
    "            dt_start = datetime(2024, 1, 1) + timedelta(days=day)\n",
    "            for i in range(48):  # 30-min intervals\n",
    "                timestamp = dt_start + timedelta(minutes=30 * i)\n",
    "                kwh = max(0, np.random.normal(0.5, 0.3))  # realistic low usage per interval\n",
    "                solar = max(0, np.random.normal(0.3, 0.2)) if 10 <= timestamp.hour <= 15 else 0\n",
    "\n",
    "                interval_records.append([cid, timestamp, kwh, solar])\n",
    "\n",
    "    df = pd.DataFrame(interval_records,\n",
    "                      columns=[\"customer_id\", \"timestamp\", \"kwh\", \"solar_export_kwh\"])\n",
    "\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/advanced_interval_data.csv\", index=False)\n",
    "    print(\"Saved advanced_interval_data.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# C. REGULATORY REPORTING DATA (ANNUAL)\n",
    "# ============================================================\n",
    "\n",
    "def simulate_regulatory_reporting():\n",
    "    data = []\n",
    "    for year in [2022, 2023, 2024]:\n",
    "        data.append({\n",
    "            \"year\": year,\n",
    "            \"total_customers\": N_CUSTOMERS,\n",
    "            \"residential_customers\": int(N_CUSTOMERS * 0.8),\n",
    "            \"business_customers\": int(N_CUSTOMERS * 0.2),\n",
    "            \"complaints\": np.random.randint(5000, 15000),\n",
    "            \"affordability_index\": np.random.uniform(0.2, 0.8),\n",
    "            \"disconnections\": np.random.randint(2000, 5000),\n",
    "            \"payment_difficulty_cases\": np.random.randint(5000, 15000),\n",
    "            \"life_support_customers\": np.random.randint(500, 1500),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/regulatory_reporting.csv\", index=False)\n",
    "    print(\"Saved regulatory_reporting.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# D. TECHNOLOGY ADOPTION DATA (Solar, EV, Batteries)\n",
    "# ============================================================\n",
    "\n",
    "def simulate_technology_adoption(customer_ids):\n",
    "    df = pd.DataFrame({\n",
    "        \"customer_id\": customer_ids,\n",
    "        \"solar_capacity_kw\": np.random.choice([0, 3, 6, 10], size=len(customer_ids), p=[0.6, 0.2, 0.15, 0.05]),\n",
    "        \"battery_capacity_kwh\": np.random.choice([0, 5, 10, 13], size=len(customer_ids), p=[0.85, 0.1, 0.03, 0.02]),\n",
    "        \"ev_plan_enrolled\": np.random.choice([0, 1], len(customer_ids), p=[0.92, 0.08]),\n",
    "        \"midday_saver_enrolled\": np.random.choice([0, 1], len(customer_ids), p=[0.7, 0.3]),\n",
    "        \"smart_meter_status\": np.random.choice([\"yes\", \"no\"], len(customer_ids), p=[0.3, 0.7]),\n",
    "    })\n",
    "\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/technology_adoption.csv\", index=False)\n",
    "    print(\"Saved technology_adoption.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# E. WESTERN POWER NETWORK DATA (substation / feeder)\n",
    "# ============================================================\n",
    "\n",
    "def simulate_network_load_profiles():\n",
    "    substations = 25\n",
    "    records = []\n",
    "\n",
    "    for s in range(substations):\n",
    "        for day in range(365):\n",
    "            for i in range(48):  # 30-min intervals\n",
    "                load = np.random.normal(10_000, 2_000)  # kW load at substation\n",
    "                records.append([s, day, i, max(0, load)])\n",
    "\n",
    "    df = pd.DataFrame(records,\n",
    "                      columns=[\"substation_id\", \"day_of_year\", \"interval_30min\", \"load_kw\"])\n",
    "\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/network_load_profiles.csv\", index=False)\n",
    "    print(\"Saved network_load_profiles.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN EVERYTHING\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billing_df = simulate_billing_system_data()\n",
    "\n",
    "    smart_meter_ids = np.random.choice(billing_df[\"customer_id\"], N_SMART_METERS, replace=False)\n",
    "    interval_df = simulate_interval_data(smart_meter_ids)\n",
    "\n",
    "    regulatory_df = simulate_regulatory_reporting()\n",
    "    tech_df = simulate_technology_adoption(billing_df[\"customer_id\"])\n",
    "    network_df = simulate_network_load_profiles()\n",
    "\n",
    "    print(\"All synthetic data generated in:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff715770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>postcode</th>\n",
       "      <th>suburb_code</th>\n",
       "      <th>tariff</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>monthly_consumption_kwh</th>\n",
       "      <th>bill_amount</th>\n",
       "      <th>payment_history_score</th>\n",
       "      <th>concession_status</th>\n",
       "      <th>connection_date</th>\n",
       "      <th>customer_tenure_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6102</td>\n",
       "      <td>100</td>\n",
       "      <td>A1</td>\n",
       "      <td>large_business</td>\n",
       "      <td>214.540771</td>\n",
       "      <td>439.460982</td>\n",
       "      <td>0.213238</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-10</td>\n",
       "      <td>10.317808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6435</td>\n",
       "      <td>384</td>\n",
       "      <td>L1</td>\n",
       "      <td>small_business</td>\n",
       "      <td>299.118885</td>\n",
       "      <td>559.032124</td>\n",
       "      <td>0.077685</td>\n",
       "      <td>0</td>\n",
       "      <td>2094-10-04</td>\n",
       "      <td>-69.802740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6860</td>\n",
       "      <td>455</td>\n",
       "      <td>M1</td>\n",
       "      <td>residential</td>\n",
       "      <td>665.443131</td>\n",
       "      <td>439.120229</td>\n",
       "      <td>0.697628</td>\n",
       "      <td>0</td>\n",
       "      <td>2053-07-30</td>\n",
       "      <td>-28.594521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6270</td>\n",
       "      <td>144</td>\n",
       "      <td>L3</td>\n",
       "      <td>medium_business</td>\n",
       "      <td>187.480672</td>\n",
       "      <td>422.690873</td>\n",
       "      <td>0.987886</td>\n",
       "      <td>0</td>\n",
       "      <td>2075-08-01</td>\n",
       "      <td>-50.613699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6106</td>\n",
       "      <td>192</td>\n",
       "      <td>K1</td>\n",
       "      <td>medium_business</td>\n",
       "      <td>44.408474</td>\n",
       "      <td>178.543948</td>\n",
       "      <td>0.546124</td>\n",
       "      <td>0</td>\n",
       "      <td>2046-07-19</td>\n",
       "      <td>-21.558904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  postcode  suburb_code tariff    customer_type  \\\n",
       "0            1      6102          100     A1   large_business   \n",
       "1            2      6435          384     L1   small_business   \n",
       "2            3      6860          455     M1      residential   \n",
       "3            4      6270          144     L3  medium_business   \n",
       "4            5      6106          192     K1  medium_business   \n",
       "\n",
       "   monthly_consumption_kwh  bill_amount  payment_history_score  \\\n",
       "0               214.540771   439.460982               0.213238   \n",
       "1               299.118885   559.032124               0.077685   \n",
       "2               665.443131   439.120229               0.697628   \n",
       "3               187.480672   422.690873               0.987886   \n",
       "4                44.408474   178.543948               0.546124   \n",
       "\n",
       "   concession_status connection_date  customer_tenure_years  \n",
       "0                  1      2014-09-10              10.317808  \n",
       "1                  0      2094-10-04             -69.802740  \n",
       "2                  0      2053-07-30             -28.594521  \n",
       "3                  0      2075-08-01             -50.613699  \n",
       "4                  0      2046-07-19             -21.558904  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d21ad69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the data from the actual synergy system to fix the simulaition parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425eb0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generate_synthetic_synergy.py\n",
    "\n",
    "Generates realistic synthetic energy datasets (billing, interval, technology, network, regulatory)\n",
    "with temperature-dependent demand curves and associated features.\n",
    "\n",
    "Outputs (in OUTPUT_DIR):\n",
    " - billing_system.parquet          (per-customer monthly / summary fields)\n",
    " - billing_system_sample.csv       (small CSV sample for quick inspection)\n",
    " - interval_data.parquet           (subset customers: 30-min intervals, multi-day or multi-year)\n",
    " - technology_adoption.parquet\n",
    " - network_load_profiles.parquet\n",
    " - regulatory_reporting.csv\n",
    "\n",
    "Configuration:\n",
    " - N_CUSTOMERS: change to 1_170_000 for full Synergy-scale simulated customer base.\n",
    " - INTERVAL_MINUTES: 30 (use 15 for higher resolution)\n",
    " - INTERVAL_DAYS: number of days to simulate for interval dataset (e.g. 365)\n",
    " - SMART_METER_SHARE: fraction of customers with interval data\n",
    "\n",
    "Notes:\n",
    " - For N_CUSTOMERS >> 200k consider running with Dask or PySpark and/or chunked Parquet writes.\n",
    " - The model is deliberately modular: tweak coefficients to match desired realism.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import uuid\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "OUTPUT_DIR = \"synthetic_synergy_data_temp_model\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "N_CUSTOMERS = 100_000        # set to 1_170_000 for full scale (watch memory / use chunking)\n",
    "SMART_METER_SHARE = 0.10     # fraction of customers with interval data (e.g. 10%)\n",
    "INTERVAL_MINUTES = 30        # 30-minute intervals\n",
    "INTERVAL_DAYS = 365          # days of interval data to simulate\n",
    "PARQUET_ENGINE = \"pyarrow\"   # ensure pyarrow installed for parquet writes\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Geographic base: sample postcodes in Perth area (6000-6999)\n",
    "POSTCODE_MIN = 6000\n",
    "POSTCODE_MAX = 6999\n",
    "N_POSTCODE_CLUSTERS = 50     # cluster postcodes to share weather patterns\n",
    "\n",
    "# Date range for interval simulation\n",
    "INTERVAL_START = datetime(2024, 1, 1)\n",
    "INTERVAL_END = INTERVAL_START + timedelta(days=INTERVAL_DAYS)\n",
    "\n",
    "# Temperature model parameters (Perth-like seasonal amplitude)\n",
    "TEMP_MEAN = 21.0             # annual mean temperature (Â°C)\n",
    "TEMP_AMP = 8.0               # amplitude of seasonal variation\n",
    "TEMP_DAY_AMP = 5.0           # daily cycle amplitude\n",
    "TEMP_NOISE_STD = 1.5         # hourly noise std dev\n",
    "\n",
    "# Comfort temperature: demand increases above/below this for cooling/heating\n",
    "COMFORT_TEMP = 20.0\n",
    "\n",
    "# Tariffs pool (example)\n",
    "TARIFFS = [\"A1\", \"L1\", \"L3\", \"R1\", \"R3\", \"M1\", \"K1\"]\n",
    "\n",
    "# EV and PV probabilities\n",
    "BASE_PV_PROB = 0.18        # fraction of households with PV\n",
    "BASE_BATTERY_PROB = 0.05\n",
    "BASE_EV_PROB = 0.08\n",
    "\n",
    "# Interval scaling (kW per interval baseline)\n",
    "BASE_DAILY_ENERGY_KWH_MEAN = 18.0   # typical household monthly ~540 kWh => daily ~18 kWh\n",
    "BASE_DAILY_ENERGY_STD = 6.0\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def make_customer_table(n_customers):\n",
    "    \"\"\"Create synthetic customer-level table with demographics and static features.\"\"\"\n",
    "    cust_ids = np.arange(1, n_customers + 1, dtype=np.int64)\n",
    "    postcodes = np.random.randint(POSTCODE_MIN, POSTCODE_MAX + 1, size=n_customers)\n",
    "    tariff = np.random.choice(TARIFFS, size=n_customers, p=[0.25,0.1,0.05,0.25,0.1,0.2,0.05])\n",
    "    cust_type = np.random.choice([\"residential\",\"small_business\",\"medium_business\",\"large_business\",\"home_business\"],\n",
    "                                 size=n_customers, p=[0.80,0.10,0.06,0.02,0.02])\n",
    "    # customer tenure (years)\n",
    "    connection_year = np.random.randint(1990, 2024, size=n_customers)\n",
    "    connection_date = pd.to_datetime(connection_year.astype(str) + \"-01-01\") + \\\n",
    "                      pd.to_timedelta(np.random.randint(0,365,size=n_customers), unit='D')\n",
    "\n",
    "    # baseline daily consumption (kWh/day) per customer\n",
    "    daily_kwh = np.clip(np.random.normal(BASE_DAILY_ENERGY_KWH_MEAN, BASE_DAILY_ENERGY_STD, size=n_customers), 2.0, 200.0)\n",
    "\n",
    "    # Temperature sensitivity coefficients: cooling (positive when temp>comfort), heating (positive when temp<comf)\n",
    "    # Make distribution: most have small sensitivity; some tech/elderly households may have larger.\n",
    "    cooling_coeff = np.random.normal(0.03, 0.015, size=n_customers)  # kWh per degC above comfort per hour\n",
    "    heating_coeff = np.random.normal(0.02, 0.01, size=n_customers)   # kWh per degC below comfort per hour\n",
    "    # ensure non-negative\n",
    "    cooling_coeff = np.clip(cooling_coeff, 0.0, None)\n",
    "    heating_coeff = np.clip(heating_coeff, 0.0, None)\n",
    "\n",
    "    # Technology adoption flags (probabilistic by postcode cluster or tariff)\n",
    "    pv_flag = np.random.binomial(1, BASE_PV_PROB, size=n_customers)\n",
    "    battery_flag = (np.random.binomial(1, BASE_BATTERY_PROB, size=n_customers) & (pv_flag==1)).astype(int)\n",
    "    ev_flag = np.random.binomial(1, BASE_EV_PROB, size=n_customers)\n",
    "\n",
    "    # PV and battery sizes (if installed)\n",
    "    pv_kw = pv_flag * np.random.choice([1.5,2.5,3.0,5.0,6.0,8.0], size=n_customers, p=[0.1,0.2,0.25,0.25,0.15,0.05])\n",
    "    battery_kwh = battery_flag * np.random.choice([3.5,5.0,10.0,13.5], size=n_customers, p=[0.4,0.35,0.15,0.1])\n",
    "\n",
    "    # EV characteristics\n",
    "    ev_kwh_per_session = ev_flag * np.random.choice([7,10,12,15,20], size=n_customers, p=[0.2,0.25,0.3,0.15,0.1])\n",
    "    ev_smart = ev_flag * np.random.binomial(1, 0.6, size=n_customers)  # share of EV owners with smart chargers\n",
    "\n",
    "    # Smart thermostats adoption\n",
    "    smart_thermostat = np.random.binomial(1, 0.18, size=n_customers)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"customer_id\": cust_ids,\n",
    "        \"postcode\": postcodes,\n",
    "        \"tariff\": tariff,\n",
    "        \"customer_type\": cust_type,\n",
    "        \"connection_date\": connection_date,\n",
    "        \"daily_kwh_mean\": daily_kwh,\n",
    "        \"cooling_coeff\": cooling_coeff,\n",
    "        \"heating_coeff\": heating_coeff,\n",
    "        \"pv_kw\": pv_kw,\n",
    "        \"battery_kwh\": battery_kwh,\n",
    "        \"ev_flag\": ev_flag,\n",
    "        \"ev_kwh_per_session\": ev_kwh_per_session,\n",
    "        \"ev_smart\": ev_smart,\n",
    "        \"smart_thermostat\": smart_thermostat,\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Weather and temperature generation\n",
    "# ----------------------------\n",
    "def generate_temperature_series(start, end, freq_minutes=60, postcode_clusters=50):\n",
    "    \"\"\"\n",
    "    Generate hourly (or freq_minutes) temperature time series per postcode cluster.\n",
    "    Returns: dict(cluster_id -> pandas.Series(datetime index))\n",
    "    Model: annual sinusoid + daily sinusoid + gaussian noise.\n",
    "    \"\"\"\n",
    "    rng = pd.date_range(start=start, end=end - timedelta(minutes=1), freq=f\"{freq_minutes}T\", tz=None)\n",
    "    n = len(rng)\n",
    "    # Day of year in radians for annual cycle\n",
    "    day_of_year = rng.dayofyear.values\n",
    "    hour_of_day = rng.hour.values + rng.minute.values / 60.0\n",
    "\n",
    "    temp_series_by_cluster = {}\n",
    "    for c in range(postcode_clusters):\n",
    "        # small variation by cluster (coastal vs inland)\n",
    "        cluster_mean = TEMP_MEAN + np.random.normal(0, 1.0)\n",
    "        cluster_amp = TEMP_AMP + np.random.normal(0, 1.5)\n",
    "        # phase shift to represent locality\n",
    "        phase_shift = np.random.uniform(0, 2*math.pi)\n",
    "        annual = cluster_amp * np.sin(2 * math.pi * (day_of_year / 365.0) + phase_shift)\n",
    "        daily = TEMP_DAY_AMP * np.sin(2 * math.pi * (hour_of_day / 24.0) + np.random.uniform(0, 2*math.pi))\n",
    "        noise = np.random.normal(0, TEMP_NOISE_STD, size=n)\n",
    "        temps = cluster_mean + annual + daily + noise\n",
    "        temp_series_by_cluster[c] = pd.Series(temps, index=rng)\n",
    "    return temp_series_by_cluster\n",
    "\n",
    "# ----------------------------\n",
    "# Solar generation model (simple) - per hour or interval\n",
    "# ----------------------------\n",
    "def simple_pv_generation(irradiance, pv_kw, system_capacity_factor=0.75):\n",
    "    \"\"\"\n",
    "    irradiance: array representing normalized irradiance [0..1] for each time index (clear-sky proxy)\n",
    "    pv_kw: PV system size in kW\n",
    "    returns: kWh produced per interval (assuming irradiance is fraction of peak and interval in hours)\n",
    "    system_capacity_factor: accounts for panel orientation, inverter efficiency\n",
    "    \"\"\"\n",
    "    # irradiance is fraction of peak. For interval length h, energy = pv_kw * irradiance * h * eff\n",
    "    return pv_kw * irradiance * system_capacity_factor\n",
    "\n",
    "def make_normalized_irradiance_index(t_index):\n",
    "    \"\"\"\n",
    "    Create normalized irradiance [0..1] for each timestamp (solar elevation proxy).\n",
    "    We'll use hour of day and day of year to shape the irradiance curve (no atmospheric modeling).\n",
    "    \"\"\"\n",
    "    doy = t_index.dayofyear.values\n",
    "    hour = t_index.hour.values + t_index.minute.values / 60.0\n",
    "    # approximate sunrise/sunset: depends on season -> use sun angle proxy\n",
    "    # simple model: peak irradiance at noon, width depends on day length (approx)\n",
    "    # daylength factor: longer in summer -> more hours with irradiance\n",
    "    daylength = 10 + 4 * np.sin(2 * np.pi * (doy / 365.0))  # approx 6-14 hours mapped to 6..14\n",
    "    # compute 'distance' from solar noon and apply gaussian shaped curve\n",
    "    solar_noon = 12.0\n",
    "    dt = (hour - solar_noon)\n",
    "    sigma = daylength / 6.0  # controls width\n",
    "    base = np.exp(-0.5 * (dt/sigma)**2)\n",
    "    # zero out during night (when |hour - 12| > daylength/2)\n",
    "    base[np.abs(dt) > (daylength/2.0)] = 0.0\n",
    "    # scale to [0,1]\n",
    "    maxv = base.max() if base.max() > 0 else 1.0\n",
    "    norm = base / maxv\n",
    "    return norm\n",
    "\n",
    "# ----------------------------\n",
    "# Interval demand generation\n",
    "# ----------------------------\n",
    "def generate_interval_profiles(customer_df, temp_series_by_cluster, interval_minutes=30, start=INTERVAL_START, days=INTERVAL_DAYS):\n",
    "    \"\"\"\n",
    "    Vectorized-ish generation of interval demand for a subset of customers.\n",
    "    Returns a DataFrame with columns: customer_id, timestamp, consumption_kwh, solar_export_kwh, battery_dispatch_kwh, ev_charging_kwh\n",
    "    Note: for memory reasons, we only generate for customers passed in customer_df (a subset for smart meters).\n",
    "    \"\"\"\n",
    "    # Create timestamps index\n",
    "    rng = pd.date_range(start=start, periods=int(24*60/interval_minutes*days), freq=f\"{interval_minutes}T\")\n",
    "    n_times = len(rng)\n",
    "    # Map each customer's postcode -> cluster id\n",
    "    # create postcode clusters mapping\n",
    "    unique_postcodes = customer_df[\"postcode\"].unique()\n",
    "    postcode_to_cluster = {}\n",
    "    # naive mapping: bucket postcodes into N_POSTCODE_CLUSTERS\n",
    "    cluster_edges = np.linspace(POSTCODE_MIN, POSTCODE_MAX+1, N_POSTCODE_CLUSTERS+1)\n",
    "    for pc in unique_postcodes:\n",
    "        idx = np.searchsorted(cluster_edges, pc, side='right') - 1\n",
    "        idx = max(0, min(N_POSTCODE_CLUSTERS-1, idx))\n",
    "        postcode_to_cluster[pc] = idx\n",
    "\n",
    "    # prepare irradiance for the timestamp index using normalized irradiance\n",
    "    irradiance = make_normalized_irradiance_index(rng)\n",
    "\n",
    "    # Precompute temperature series per cluster for the timestamps\n",
    "    temp_matrix = np.zeros((N_POSTCODE_CLUSTERS, n_times), dtype=np.float32)\n",
    "    for c in range(N_POSTCODE_CLUSTERS):\n",
    "        s = temp_series_by_cluster[c]\n",
    "        # Align s to rng index (s might have same freq)\n",
    "        temp_matrix[c, :] = s.reindex(rng, method=\"nearest\").values\n",
    "\n",
    "    # For each customer, compute baseline interval consumption distribution\n",
    "    # We'll create a daily shape profile (24h) normalized to 1, then scale by customer's daily kWh mean\n",
    "    # daily profile: low at night, morning peak, daytime dip, evening peak\n",
    "    hour = rng.hour + rng.minute / 60.0\n",
    "    # create normalized load shape for a single day (periodic every 24h)\n",
    "    # baseline shape (arbitrary but plausible)\n",
    "    base_shape = 0.6 + 0.4*np.exp(-0.5*((hour-7)/2.0)**2) + 0.8*np.exp(-0.5*((hour-19)/3.0)**2) \\\n",
    "                 + 0.2*np.exp(-0.5*((hour-13)/3.0)**2)\n",
    "    # normalize per day: we want sum over 24h to equal 1 for per-day scaling (account for interval length)\n",
    "    # compute per-interval hours\n",
    "    hrs_per_interval = interval_minutes / 60.0\n",
    "    # group into days to normalize per day: we can compute normalization factor per day index\n",
    "    # simpler: compute a typical day sum over 24h cycles\n",
    "    # find positions for one representative day (first 24h)\n",
    "    rep_day_mask = (rng.date == rng.date[0])\n",
    "    # But easier: compute normalization per 24h cycle by reshaping\n",
    "    # create array grouped by day\n",
    "    days_count = int(n_times / (24*60/interval_minutes))\n",
    "    shape_by_day = base_shape.reshape(days_count, -1)  # (days_count, intervals_per_day)\n",
    "    # compute normalization (sum across intervals * hrs_per_interval)\n",
    "    norm_per_day = shape_by_day.sum(axis=1) * hrs_per_interval\n",
    "    # we'll divide each day's entries by that day's normalization\n",
    "    normalized_shape = np.repeat((shape_by_day / norm_per_day[:, None]).reshape(-1), 1)\n",
    "\n",
    "    # Prepare output container as list of DataFrames per chunk\n",
    "    rows = []\n",
    "    # To avoid huge memory use, process customers in chunks\n",
    "    CHUNK = 2000  # tune based on memory\n",
    "    cust_ids = customer_df[\"customer_id\"].values\n",
    "    n_cust = len(cust_ids)\n",
    "    for i0 in range(0, n_cust, CHUNK):\n",
    "        i1 = min(n_cust, i0 + CHUNK)\n",
    "        chunk = customer_df.iloc[i0:i1]\n",
    "        # For each customer in chunk, compute consumption over time vectorized-ish\n",
    "        # Compute daily kwh to interval kwh: daily_kwh_mean * normalized_shape * randomness * temperature effect\n",
    "        # Build matrix: (n_customers_in_chunk, n_times)\n",
    "        ck = len(chunk)\n",
    "        # repeat arrays\n",
    "        daily_kwh = chunk[\"daily_kwh_mean\"].values.reshape(-1,1)  # (ck,1)\n",
    "        # baseline profile repeated\n",
    "        profile = normalized_shape.reshape(1,n_times)  # (1, n_times)\n",
    "        # randomness per customer and per time\n",
    "        noise = np.random.normal(1.0, 0.05, size=(ck, n_times))\n",
    "\n",
    "        # temperature effect: for each customer, map to cluster temps and compute deviation from comfort\n",
    "        cluster_ids = np.array([postcode_to_cluster[pc] for pc in chunk[\"postcode\"].values], dtype=np.int32)\n",
    "        temp_for_customers = temp_matrix[cluster_ids, :]  # (ck, n_times)\n",
    "        # cooling effect: when temp > COMFORT_TEMP\n",
    "        temp_dev_cooling = np.clip(temp_for_customers - COMFORT_TEMP, 0, None)\n",
    "        temp_dev_heating = np.clip(COMFORT_TEMP - temp_for_customers, 0, None)\n",
    "        # per-hour effect convert coeffs (kWh per degC per hour) to per-interval\n",
    "        cooling_coeff = chunk[\"cooling_coeff\"].values.reshape(-1,1)\n",
    "        heating_coeff = chunk[\"heating_coeff\"].values.reshape(-1,1)\n",
    "        temp_effect = (cooling_coeff * temp_dev_cooling + heating_coeff * temp_dev_heating) * (interval_minutes/60.0)\n",
    "\n",
    "        # compute base consumption\n",
    "        consumption_kwh = daily_kwh * profile * noise + temp_effect\n",
    "\n",
    "        # solar export per customer: compute irradiance * pv_kw * eff * interval_hours\n",
    "        pv_kw = chunk[\"pv_kw\"].values.reshape(-1,1)\n",
    "        irradiance_arr = irradiance.reshape(1, -1)  # (1,n_times)\n",
    "        pv_eff = 0.75\n",
    "        solar_kwh = pv_kw * irradiance_arr * pv_eff * (interval_minutes/60.0)\n",
    "\n",
    "        # battery simple dispatch: households with battery will absorb midday solar up to battery capacity then discharge in evening\n",
    "        battery_kwh = np.zeros_like(consumption_kwh)\n",
    "        has_batt = chunk[\"battery_kwh\"].values\n",
    "        # simple heuristic per-customer battery usage across day: charge during intervals where solar>0, discharge during evening peak hours (17-21)\n",
    "        # Implement per-day dispatch in simplistic manner to avoid complex optimization\n",
    "        if np.any(has_batt > 0):\n",
    "            batt_caps = chunk[\"battery_kwh\"].values\n",
    "            for idx, cap in enumerate(batt_caps):\n",
    "                if cap <= 0:\n",
    "                    continue\n",
    "                # per-day schedule: charge from solar up to cap, then discharge uniformly in 17-21 hours\n",
    "                # compute per-day arrays\n",
    "                # reshape to days x intervals_per_day\n",
    "                ints_per_day = int(24*60/INTERVAL_MINUTES)\n",
    "                cons_daily = consumption_kwh[idx].reshape(days_count, ints_per_day)\n",
    "                solar_daily = solar_kwh[idx].reshape(days_count, ints_per_day)\n",
    "                batt_daily = np.zeros_like(cons_daily)\n",
    "                for d in range(days_count):\n",
    "                    available_charge = solar_daily[d].sum()  # naive\n",
    "                    charge_amt = min(cap, available_charge * 0.9)  # fraction can be stored\n",
    "                    # charge during daytime intervals proportional to solar\n",
    "                    if solar_daily[d].sum() > 0:\n",
    "                        charge_profile = solar_daily[d] / solar_daily[d].sum() * charge_amt\n",
    "                    else:\n",
    "                        charge_profile = np.zeros_like(solar_daily[d])\n",
    "                    # discharge in evening intervals (17:00-21:00)\n",
    "                    evening_mask = ((np.arange(ints_per_day) * (INTERVAL_MINUTES/60.0)) >= 17) & ((np.arange(ints_per_day)*(INTERVAL_MINUTES/60.0)) < 21)\n",
    "                    if evening_mask.sum() > 0:\n",
    "                        discharge_profile = np.zeros(ints_per_day)\n",
    "                        discharge_profile[evening_mask] = charge_amt / evening_mask.sum()\n",
    "                    else:\n",
    "                        discharge_profile = np.zeros(ints_per_day)\n",
    "                    batt_daily[d] = - discharge_profile + charge_profile  # positive = charge, negative = discharge (net to grid)\n",
    "                battery_kwh[idx] = batt_daily.reshape(-1)\n",
    "            # battery_kwh now contains positive for charge (consumes from grid) and negative for discharge (exports)\n",
    "        # EV charging model: for ev owners, add charging demand typically at evening/night unless smart charging (shiftable)\n",
    "        ev_charging_kwh = np.zeros_like(consumption_kwh)\n",
    "        ev_flag_arr = chunk[\"ev_flag\"].values\n",
    "        ev_session = chunk[\"ev_kwh_per_session\"].values\n",
    "        ev_smart = chunk[\"ev_smart\"].values\n",
    "        # simplistic model: each EV does 0-1 sessions per day (prob 0.5), arrives at 18:00 and charges until filled unless smart scheduler shifts to off-peak\n",
    "        ints_per_day = int(24*60/INTERVAL_MINUTES)\n",
    "        for idx in range(ck):\n",
    "            if ev_flag_arr[idx] == 0:\n",
    "                continue\n",
    "            for d in range(days_count):\n",
    "                didx_start = d*ints_per_day + int((18.0 / 24.0) * ints_per_day)  # 18:00 index approximation\n",
    "                energy_needed = ev_session[idx]\n",
    "                if ev_smart[idx] == 1:\n",
    "                    # smart behavior: shift to off-peak (assume off-peak midnight-6) or split\n",
    "                    # we'll prefer midnight start if tariff encourages\n",
    "                    start_idx = d*ints_per_day + int((1.0 / 24.0) * ints_per_day)  # 01:00\n",
    "                else:\n",
    "                    start_idx = didx_start\n",
    "                # simple charging rate 7 kW -> per interval kWh\n",
    "                per_interval_kwh = 7.0 * (INTERVAL_MINUTES/60.0)\n",
    "                intervals_needed = int(max(1, math.ceil(energy_needed / per_interval_kwh)))\n",
    "                for t in range(intervals_needed):\n",
    "                    t_idx = start_idx + t\n",
    "                    if t_idx < (d+1)*ints_per_day:\n",
    "                        ev_charging_kwh[idx, t_idx] += per_interval_kwh\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        # Now compute net grid consumption: base consumption + battery_charge (positive) - battery_discharge (negative) + ev_charging - solar_export (solar exported if production exceeds consumption+charge)\n",
    "        net_before_export = consumption_kwh + np.maximum(battery_kwh, 0) + ev_charging_kwh\n",
    "        # solar_export: if solar production > net_before_export, export = surplus, else zero\n",
    "        solar_export = np.maximum(solar_kwh - net_before_export, 0.0)\n",
    "        # final grid consumption = net_before_export - min(solar_kwh, net_before_export)\n",
    "        grid_consumption = net_before_export - np.minimum(solar_kwh, net_before_export)\n",
    "\n",
    "        # Prepare DataFrame rows for chunk\n",
    "        # We will store: customer_id, timestamp, grid_kwh, solar_kwh, solar_export_kwh, battery_kwh, ev_charging_kwh\n",
    "        # To keep file size manageable, write per-customer compressed or per-chunk DataFrames and append to Parquet\n",
    "        for j, cust in enumerate(chunk[\"customer_id\"].values):\n",
    "            row_df = pd.DataFrame({\n",
    "                \"customer_id\": np.repeat(int(cust), n_times),\n",
    "                \"timestamp\": rng,\n",
    "                \"grid_consumption_kwh\": grid_consumption[j],\n",
    "                \"solar_kwh\": solar_kwh[j],\n",
    "                \"solar_export_kwh\": solar_export[j],\n",
    "                \"battery_dispatch_kwh\": battery_kwh[j],   # positive charge, negative discharge\n",
    "                \"ev_charging_kwh\": ev_charging_kwh[j],\n",
    "            })\n",
    "            # append to Parquet file (row-group/chunk)\n",
    "            out_file = os.path.join(OUTPUT_DIR, \"interval_data.parquet\")\n",
    "            # Use append mode by writing in first chunk then append\n",
    "            if (i0 == 0 and j == 0 and not os.path.exists(out_file)):\n",
    "                row_df.to_parquet(out_file, engine=PARQUET_ENGINE, index=False)\n",
    "            else:\n",
    "                # append by reading/writing with pyarrow backend would be better; pandas to_parquet does not support append - do chunked single-file approach:\n",
    "                # Instead, write each chunk as separate file and later concatenate (recommended for large-scale)\n",
    "                chunk_file = os.path.join(OUTPUT_DIR, f\"interval_chunk_{i0}_{j}.parquet\")\n",
    "                row_df.to_parquet(chunk_file, engine=PARQUET_ENGINE, index=False)\n",
    "        # end chunk loop\n",
    "        print(f\"Processed customers {i0} .. {i1-1} for interval profiles\")\n",
    "\n",
    "    # After generating many small parquet chunks, optionally concatenate them into one Parquet dataset folder.\n",
    "    # For now, we leave many chunk files in OUTPUT_DIR; user can merge using pyarrow.dataset or fastparquet if desired.\n",
    "    return True\n",
    "\n",
    "# ----------------------------\n",
    "# Network load profiles (aggregated)\n",
    "# ----------------------------\n",
    "def simulate_network_load_profiles(postcode_customer_df, interval_minutes=30, start=INTERVAL_START, days=INTERVAL_DAYS):\n",
    "    \"\"\"\n",
    "    Aggregate simulated network load at zone substation level by summing a sampled fraction of customers.\n",
    "    We'll create synthetic zone substations and produce 30-min aggregated load time series.\n",
    "    \"\"\"\n",
    "    rng = pd.date_range(start=start, periods=int(24*60/interval_minutes*days), freq=f\"{interval_minutes}T\")\n",
    "    ints_per_day = int(24*60/interval_minutes)\n",
    "    days_count = int(len(rng) / ints_per_day)\n",
    "    substations = 50\n",
    "    # choose random assignment of postcodes to substations\n",
    "    unique_postcodes = np.unique(postcode_customer_df[\"postcode\"].values)\n",
    "    pc_to_sub = {pc: int(hash(str(pc)) % substations) for pc in unique_postcodes}\n",
    "\n",
    "    # baseline load per substation per interval (kW)\n",
    "    records = []\n",
    "    for s in range(substations):\n",
    "        # create base profile using hour profile times a substation scale factor\n",
    "        hour = rng.hour + rng.minute/60.0\n",
    "        base_profile = 1000 * (1.0 + 0.5*np.exp(-0.5*((hour-7)/2.0)**2) + 0.8*np.exp(-0.5*((hour-19)/3.0)**2))\n",
    "        scale = np.random.uniform(0.5, 2.0)  # vary by substation customer density\n",
    "        noise = np.random.normal(0, 100, size=len(rng))\n",
    "        load_kw = np.maximum(0, base_profile * scale + noise)\n",
    "        df = pd.DataFrame({\n",
    "            \"substation_id\": np.repeat(s, len(rng)),\n",
    "            \"timestamp\": rng,\n",
    "            \"load_kw\": load_kw\n",
    "        })\n",
    "        # write per-substation parquet to avoid memory pressure\n",
    "        df.to_parquet(os.path.join(OUTPUT_DIR, f\"network_substation_{s}.parquet\"), engine=PARQUET_ENGINE, index=False)\n",
    "        records.append({\"substation_id\": s, \"avg_load_kw\": load_kw.mean()})\n",
    "    # write summary\n",
    "    pd.DataFrame(records).to_csv(os.path.join(OUTPUT_DIR, \"network_substation_summary.csv\"), index=False)\n",
    "    return True\n",
    "\n",
    "# ----------------------------\n",
    "# Regulatory reporting simulation\n",
    "# ----------------------------\n",
    "def simulate_regulatory_reporting(n_customers):\n",
    "    # simplified ERA-like indicators for 3 years\n",
    "    years = [2022, 2023, 2024]\n",
    "    rows = []\n",
    "    for y in years:\n",
    "        rows.append({\n",
    "            \"year\": y,\n",
    "            \"total_customers\": n_customers,\n",
    "            \"residential\": int(n_customers * 0.80),\n",
    "            \"business\": int(n_customers * 0.20),\n",
    "            \"complaints\": int(np.random.randint(4000, 20000)),\n",
    "            \"affordability_index\": round(np.random.uniform(0.2, 0.8), 3),\n",
    "            \"disconnections\": int(np.random.randint(1000, 8000)),\n",
    "            \"payment_difficulty\": int(np.random.randint(4000, 20000)),\n",
    "            \"life_support_customers\": int(np.random.randint(200, 2000)),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, \"regulatory_reporting.csv\"), index=False)\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Billing (monthly summary) generation\n",
    "# ----------------------------\n",
    "def generate_billing_summary(customer_df, months=12):\n",
    "    \"\"\"\n",
    "    Generate monthly billing system summary for each customer by sampling consumption from daily means and tariffs.\n",
    "    Writes billing_system.parquet and a small CSV sample.\n",
    "    \"\"\"\n",
    "    # simulate month-by-month totals for 'months' months\n",
    "    months_idx = pd.date_range(end=datetime(2024,12,31), periods=months, freq='M')\n",
    "    rows = []\n",
    "    for i, row in customer_df.iterrows():\n",
    "        cust_id = int(row[\"customer_id\"])\n",
    "        base_daily = float(row[\"daily_kwh_mean\"])\n",
    "        tariff = row[\"tariff\"]\n",
    "        concession = 0 if np.random.rand() > 0.12 else 1\n",
    "        for m in months_idx:\n",
    "            # seasonal multiplier by month (simple)\n",
    "            month_season_factor = 1.0 + 0.15 * np.sin(2*math.pi*(m.month/12.0))\n",
    "            # random monthly variation\n",
    "            monthly_kwh = base_daily * 30 * month_season_factor * np.random.normal(1.0, 0.08)\n",
    "            bill_amount = monthly_kwh * (0.30 + np.random.normal(0.0, 0.05))  # approximate $/kWh\n",
    "            rows.append({\n",
    "                \"customer_id\": cust_id,\n",
    "                \"invoice_month\": m.strftime(\"%Y-%m\"),\n",
    "                \"monthly_kwh\": monthly_kwh,\n",
    "                \"bill_amount\": round(bill_amount, 2),\n",
    "                \"tariff\": tariff,\n",
    "                \"concession\": concession,\n",
    "                \"postcode\": row[\"postcode\"]\n",
    "            })\n",
    "    billing_df = pd.DataFrame(rows)\n",
    "    # write parquet (partition by invoice_month if desired)\n",
    "    billing_df.to_parquet(os.path.join(OUTPUT_DIR, \"billing_system.parquet\"), engine=PARQUET_ENGINE, index=False)\n",
    "    billing_df.sample(min(5000, len(billing_df))).to_csv(os.path.join(OUTPUT_DIR, \"billing_system_sample.csv\"), index=False)\n",
    "    return billing_df\n",
    "\n",
    "# ----------------------------\n",
    "# Main script orchestration\n",
    "# ----------------------------\n",
    "def main():\n",
    "    print(\"Generating customer table...\")\n",
    "    cust_df = make_customer_table(N_CUSTOMERS)\n",
    "    cust_df.to_parquet(os.path.join(OUTPUT_DIR, \"customers.parquet\"), engine=PARQUET_ENGINE, index=False)\n",
    "    print(f\"Saved customers.parquet ({len(cust_df)} rows)\")\n",
    "\n",
    "    # generate postcode clusters temperature series\n",
    "    print(\"Generating synthetic temperature series for postcode clusters...\")\n",
    "    temp_series_by_cluster = generate_temperature_series(start=INTERVAL_START, end=INTERVAL_END, freq_minutes=INTERVAL_MINUTES, postcode_clusters=N_POSTCODE_CLUSTERS)\n",
    "    # Optionally persist temp series for reproducibility\n",
    "    # For speed, store cluster means as csv (not the full timeseries)\n",
    "    cluster_stats = []\n",
    "    for cid, s in temp_series_by_cluster.items():\n",
    "        cluster_stats.append({\"cluster_id\": cid, \"mean_temp\": float(s.mean()), \"std_temp\": float(s.std())})\n",
    "    pd.DataFrame(cluster_stats).to_csv(os.path.join(OUTPUT_DIR, \"temp_cluster_stats.csv\"), index=False)\n",
    "\n",
    "    # Technology adoption table\n",
    "    print(\"Saving technology adoption...\")\n",
    "    tech_cols = [\"customer_id\",\"pv_kw\",\"battery_kwh\",\"ev_flag\",\"ev_kwh_per_session\",\"ev_smart\",\"smart_thermostat\"]\n",
    "    tech_df = cust_df[[\"customer_id\",\"pv_kw\",\"battery_kwh\",\"ev_flag\",\"ev_kwh_per_session\",\"ev_smart\",\"smart_thermostat\"]].copy()\n",
    "    tech_df.to_parquet(os.path.join(OUTPUT_DIR, \"technology_adoption.parquet\"), engine=PARQUET_ENGINE, index=False)\n",
    "    print(\"Saved technology_adoption.parquet\")\n",
    "\n",
    "    # Generate billing monthly summaries\n",
    "    print(\"Generating billing monthly summaries (this may take a while depending on N_CUSTOMERS)...\")\n",
    "    billing_df = generate_billing_summary(cust_df, months=12)\n",
    "    print(\"Saved billing_system.parquet and sample CSV\")\n",
    "\n",
    "    # Interval data for smart meter customers (subset)\n",
    "    n_smart = int(N_CUSTOMERS * SMART_METER_SHARE)\n",
    "    smart_customers = cust_df.sample(n=n_smart, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    smart_customers.to_parquet(os.path.join(OUTPUT_DIR, \"smart_customers.parquet\"), engine=PARQUET_ENGINE, index=False)\n",
    "    print(f\"Selected {n_smart} customers for interval simulation and saved smart_customers.parquet\")\n",
    "\n",
    "    print(\"Generating network load profiles (per-substation parquet files)...\")\n",
    "    simulate_network_load_profiles(cust_df)\n",
    "\n",
    "    print(\"Generating regulatory reporting...\")\n",
    "    simulate_regulatory_reporting(N_CUSTOMERS)\n",
    "\n",
    "    print(\"Starting interval profile generation (this can be slow; check OUTPUT_DIR for chunked parquet files)...\")\n",
    "    # For large datasets consider running generate_interval_profiles in parallel or with Dask\n",
    "    generate_interval_profiles(smart_customers, temp_series_by_cluster, interval_minutes=INTERVAL_MINUTES, start=INTERVAL_START, days=INTERVAL_DAYS)\n",
    "\n",
    "    print(\"All datasets generated in:\", OUTPUT_DIR)\n",
    "    print(\"Note: interval generation writes many chunk files. Use pyarrow.dataset or pandas.concat (careful with memory) to assemble a single dataset if needed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
